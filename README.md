## How to Run This Project

### Quick Start

#### Prerequisites

- **Python 3.11+**  
- **pip** or **conda** for package management  

---

### Local Installation

1. **Clone the repository**
   ```bash
   git clone <your-repository>
   cd Project-SDE-Lotka-Volterra-model
   ```

2. **Create a virtual environment (recommended)**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
---
# About this project
## Physics‚ÄìInformed Neural SDEs: Implementation of GBM and Heston

## Based on the Stochastic Physics-Informed Neural ODEs methodology (O‚ÄôLeary, Paulson & Mesbah, 2023)

### 1. Introduction

This repository implements a simplified and pedagogical version of the Stochastic Physics-Informed Neural Ordinary Differential Equations (SPIN-ODEs) framework originally proposed by O‚ÄôLeary, Paulson, and Mesbah (2023).

The original work introduces a general methodology for training neural models to learn stochastic dynamics while enforcing physical or structural constraints derived from stochastic differential equations (SDEs).

In our case, we extend the base example provided in the reference code ‚Äîthe Lotka‚ÄìVolterra system‚Äî and implement two additional experiments based on widely used models in quantitative finance:

Geometric Brownian Motion (GBM)

Heston Stochastic Volatility Model

The goal is to demonstrate that the framework is modular, allowing the underlying dynamical system to be replaced without altering the core training architecture.

### 2. Motivation

The primary motivation behind this project is to explore whether the SPIN-ODE methodology can:

Reliably separate drift and diffusion within a neural training pipeline,

Learn parameters and trajectories even when the noise source is not directly observable,

Extend beyond the Lotka‚ÄìVolterra example while preserving numerical stability, stochastic fidelity, and computational efficiency.

This repository serves as a pedagogical laboratory, where several elements of the original framework are simplified to facilitate study, visualization, and reproducibility.

### 3. Connection to the Original Work

Our implementation is built upon the reference application provided by Jared O‚ÄôLeary, associated with the SPIN-ODE publication:

O'Leary, J., Paulson, J.A., & Mesbah, A. (2023).
Stochastic Physics-Informed Neural Ordinary Differential Equations.
University of California, Berkeley / The Ohio State University.

In particular:

We preserve the modular training structure,

We follow the same philosophy of splitting learning into train_g1 (drift) and train_g2 (diffusion),

We adapt the data generation pipeline to support GBM and Heston,

We simplify selected components to make the experiments easier to interpret and replicate.

### 4. General Framework Structure
#### 4.1 Drift‚ÄìDiffusion Decomposition

Following the methodology of O‚ÄôLeary et al., the training pipeline is divided into two complementary phases:

Phase G1 (train_g1)

Learning the deterministic component of the SDE (the drift) using observed trajectories.

Phase G2 (train_g2)

Learning the stochastic component (the diffusion) using the residual increments between the learned drift and the true state transitions.

This decomposition is valid only when the SDE admits a representation of the form:


$$
\begin{aligned}
dX_t &= f(X_t,t)\,dt + g(X_t,t)\,dW_t
\end{aligned}
$$

and when an explicit Euler‚ÄìMaruyama simulator is available so that the drift and diffusion components can be separated from synthetic data.

### 5. Implemented Models
5.1 Geometric Brownian Motion (GBM)
$$
\begin{aligned}
dS_t &= \mu S_t\,dt + \sigma S_t\,dW_t
\end{aligned}
$$‚Äã

Relevant characteristics:

Drift and diffusion are fully separable.

Serves as a baseline for numerical stability and accuracy of the framework.

Enables direct comparison with the closed-form analytical solution.

### 5.2 Heston Model
$$
\begin{aligned}
dS_t &= \mu S_t\,dt + \sqrt{v_t}\,S_t\,dW_t^{(1)},\\
dv_t &= \kappa(\theta - v_t)\,dt + \xi\sqrt{v_t}\,dW_t^{(2)}
\end{aligned}
$$
	‚Äã
with correlation

$$
\begin{aligned}
\rho &= \mathrm{corr}(dW_t^{(1)}, dW_t^{(2)})
\end{aligned}
$$

Key features:

Two-dimensional stochastic system with structured diffusion.

Requires consistency between correlated noise sources.

Tests the framework‚Äôs ability to learn coupled stochastic dynamics.

### 6. Code Architecture

The repository is structured as follows:
   ```bash
   /SPINODE/LVE/         # Generation and storage of simulated trajectories for Lotka-Volterra model
   /SPINODE/GBM/         # Generation and storage of simulated trajectories for  Black-Scholes model
   /SPINODE/Heston/      # Generation and storage of simulated trajectories for Heston Stochastic Volatility model

   /SPINODE/NAMEMODEL/   # Neural network models for drift and diffusion of NAMEMODEL (LVE, GB or Heston)
   /SPINODE/train.py     # train_g1.py, train_g2.py
   /SPINODE/run.ipynb    # Scripts for running GBM and Heston experiments
   /SPINODE/utils.py     # Auxiliary utilities
   README.md             # This document
   /Other_tries/ ...
   ```

#### 6.1 train_g1

This module trains the drift by fitting:

$$
\begin{aligned}
\widehat{f}_{\theta}(x,t) &\approx \frac{X_{t+\Delta t}-X_t}{\Delta t}
\end{aligned}
$$

#### 6.2 train_g2

With the drift already learned, the module estimates the noise structure, recovering:

$$
\begin{aligned}
\widehat{g}_{\phi}(x,t) &\approx 
\frac{X_{t+\Delta t}-X_t-\widehat{f}_{\theta}(x,t)\,\Delta t}{\sqrt{\Delta t}}
\end{aligned}
$$

### 7. Expected Results

In GBM, the method recovers parameters close to the true 
ùúá
Œº and 
ùúé
œÉ.

In Heston, the method reproduces coherent trajectories and captures the volatility-driven noise structure.

The approach is numerically stable and extensible to other separable drift‚Äìdiffusion models.

### 8. Known Limitations

This methodology is not directly applicable when:

Drift and diffusion cannot be cleanly separated.

The diffusion term is not driven by Gaussian noise.

The dynamics involve jumps or non-Euler‚ÄìMaruyama noise (e.g., L√©vy processes).

The system exhibits degenerate diffusion that cannot be extracted from finite-difference increments.

### 9. Bibliographic Reference

This project draws upon and integrates ideas from the following works:

- **O'Leary, J., Paulson, J.A., & Mesbah, A. (2023).**  
  *Stochastic Physics-Informed Neural Ordinary Differential Equations.*  
  University of California, Berkeley / The Ohio State University.

- **El Janati Elidrissi, Y., & Efstathiadis, G. (2023).**  
  *PINN-Based SDE Solver.*  
  Harvard T.H. Chan School of Public Health.

- **Olgu√≠n, D. (2024).**  
  *The Math Behind the Magic: Neural Networks, Theory and Practice.*  
  Encuentro Nacional de Ingenier√≠a Matem√°tica 2024,  
  with J. Fontbona, J. Maass, and C. Mu√±oz.


